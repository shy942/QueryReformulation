\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmicx}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[numbers]{natbib} 
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Improved Bug Localization using Association Mapping and Information Retrieval\\
%{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
%/should not be used}
%\thanks{Identify applicable funding agency here. If none, delete this.}
}

%\author{\IEEEauthorblockN{Shamima Yeasmin~~~Mohammad Masudur Rahman~~~Chanchal K. Roy~~~ Kevin A. Schneider}
%\IEEEauthorblockA{\textit{Department of Computer Science} \\
%\textit{ University of Saskatchewan}\\
%Saskatoon, Canada \\
%shy942@mail.usask.ca, \{masud.rahman, chanchal.roy, kevin.schneider\}@usask.ca}
%\and
%\IEEEauthorblockN{Mohammad Masudur Rahman}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address}
%\and
%\IEEEauthorblockN{Chanchal K. roy}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address}
%\and
%\IEEEauthorblockN{Kevin A. Schneider}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address}

%}

\maketitle

\begin{abstract}
Bug localization is one of the most challending tasks undertaken by the developers during software maintenance.
Most of the existing studies rely on lexical similarity between the bug reports and source code for bug localization.
Unfortunately, such similarity always does not exist, and these studies suffer from vocabulary mismatch issues.
In this paper, we propose a bug localization technique that (1) not only uses lexical similarity between a given bug report and source code documents  
but also (2) exploits the association between keywords from the past reports and source files from corresponding changed code.
Experiments using a collection of ~3973 bug reports show that our technique can locate buggy files with a Top-10 accuracy of 64.05\% and a mean reciprocal rank@10 of 0.37 and a mean precision average@10 of 39\%, which are highly promising. 
Comparison with the state-of-the-art techniques
and their variants report that our technique can improve 12\% in
MAP@10 and 12.6\% in Top-5 accuracy over the state-of-the-art.
\end{abstract}

\begin{IEEEkeywords}
bug localization, bug report, source code, information retrieval, keyword-source code association
\end{IEEEkeywords}

\section{Introduction}
Bug localization is the process of locating the source code that needs to be changed in order to fix a given bug. 
Locating buggy files is time-consuming and costly in terms of efforts \cite{Wang}. This is even more challenging especially for the large software systems. Thus, effective methods for locating bugs automatically from bug reports are highly desirable. 
Traditional bug localization techniques accept a bug report and a subject system as inputs and produce a list of entities (e.g., classes, methods) for the bug report. Information retrieval based techniques localize software bugs by considering relevance between bug report and source code in terms of their lexical similarity. %and return a ranked list of source code entities which may contain the bug. 
Hence, these techniques are likely to be affected by the quality and content of a submitted query. If a query contains inadequate information, then the retrieved results might not be relevant at all. Thus, simply relying on lexical similarity might not be helpful. 
%However, static bug localization technique has an advantage over dynamic technique, as it does not require any working subject system rather it can apply on any stage of a system. 
In this work, we additionally consider an association between bug reports and their corresponding changed source documents extracted from commit logs for the bug localization.

\citet{Jian} first propose BugLocator based on revised Vector Space Model (rVSM) that uses the lexical similarity between bug report texts and source code. \citet{Saha} capture the structures of both bug reports and source documents, and then apply structured information retrieval for bug localization. \citet{Moreno} consider structural similarity between stack traces in the bug report and the project document structures for bug localization. However, all of these studies share one common limitation.
%In existing studies, information retrieval techniques  \cite{Jian} \cite{Saha} \cite{Moreno} \cite{Anh} \cite{Lukins} are applied to automatically search for relevant buggy files based on a given bug report. 
%In case of Text Retrieval (TR) based approaches, \citet{Jian} propose BugLocator based on revised Vector Space Model (rVSM), which requires the information extracted from both the bug reports and the source codes.
%Due to the fuzzy nature of information retrieval, textually matching bug reports against source
%files may have to concede noise (i.e., less relevant but accidentally matched words). Therefore, by treating
%files as single units or favoring large files, this technique \cite{Jian} is more
%likely to be affected by the noise in large files.
%One of the issue associated with TR-based technique is treating source codes as flat text. 
%Here, exploiting the structure of source code can be an useful way to improve bug localization accuracy.
%So, \citet{Saha} present BLUiR, which retrieve structural information from code constructs.
%However, bug reports often contain stack-trace information, which may provide direct clues for possible faulty files. Most existing approaches directly treat the bug descriptions as plain texts and do not explicitly consider stack-trace information. Here, \citet{Moreno} combine both stack trace similarity and textual similarity to retrieve potential code elements. 
%To deal with two issues associated with source code structure and stack trace information, 
%\citet{Chu}  proposed  a technique that use segmentation i.e., divides each source code file into segments and use stack-trace analysis, which significantly improve the performance of BugLocator.
%LDA topic-based approaches \cite{Anh} \cite{Lukins} assume that the textual contents of a bug report and it's corresponding buggy source files share some technical aspects of the system.
%Therefore, they develop topic model that represents those technical aspects as topic. 
%However, existing bug localization approaches applied on small-scale data for evaluation so far.
%Besides the problem of small-scale evaluations, the performance of the existing bug localization methods can be further improved too. For example, using Latent Dirichlet Allocation (LDA), only buggy files for 22\% of Eclipse 3.1 bug reports are ranked in the top 10 [25]. 
%But, now it is an important research question to know how effective these approaches are for locating bugs in large-scale (i.e., big data).
%In the field of query processing
%\citet{Sisman}  proposed a method that examines the files retrieved for the initial query supplied by a user and then selects from these files only those additional terms that are in close proximity to the terms in the initial query. Their \cite{Sisman} experimental evaluation on two large software projects using more than 4,000 queries showed that the proposed approach leads to significant improvements for bug localization and outperforms the well-known QR methods
%in the literature.
First, most of the IR-based bug localization techniques simply rely on only lexical similarity for retrieving relevant documents from the codebase. Hence, the search queries constructed from a given bug report should contain keywords similar to code terms. Unfortunately, bug reports could be of low quality and could miss the appropriate keywords. This also warrants for domain expertise on a given project, which can always not be guaranteed. 
%Second, when treating files as single units, they are affected by the noise of the large files. This issue also reduces the performance of the applicability of exploiting lexical similarity measure.


In this paper, we propose a bug localization approach namely BLUAMIR that not only considers lexical similarity but also captures implicit association between keywords and the source code. 
We consider the implicit association between keywords extracted from previously fixed bug reports and their corresponding changed code. The idea is to capture the  relationship without direct vocabulary match.  
Thus, our proposed approach addresses the vocabulary mismatch problem using a keyword-source association constructed from fixed bug information. 
Our proposed technique also overcomes the issue of locating large files. Construction of association mapping between large source documents and an query does not require any direct vocabulary matching. Thus, our approach is not affected by the noise from large source documents and can rank the documents better than the approaches based on lexical similarity only.
%The vocabulary between keywords extracted from a previously fixed bug report and its corresponding source code can not be matched, but can be linked in an association mapping. Thus, by exploiting this kind of relationship can aid locating future bug even if there exist no matched keywords between keywords from that bug and recommended source code.
%We compare the performance of our proposed tool with a state of the art bug localization techniques \cite{Jian}.

We evaluate our technique in several different dimensions using
three widely used performance metrics and 3973 bug reports (i.e.,
queries) from three open source subject systems. 
First, we evaluate in terms of the performance metrics, contrast with two replicated baseline, Vector Space Model(VSM) and BugLocator \cite{Jian}.
BLuAMIR localizes bugs with in average 9\% higher accuracy (i.e.,
Hit@10), 12\% higher precision (i.e., MAP@10) and 11\%
higher result ranks (i.e., MRR@10) than the replicated baseline VSM (Section \ref{RQ1answer}).
BLuAMIR also localizes bugs with in avaerage 57\% higher accuracy (i.e., Hit@10), 81\% higher precision(i.e., MAP@10) and 94\% higher resul ranks(i.e., MRR@10) than replicated BugLocator \cite{Jian} (section \ref{RQ1answer})
Second, we compare our technique with a state of the art approach \cite{Jian}
and our technique can improve 12\% in MAP@10
and 12.6\% in Top-5 accuracy over the state-of-the-art (Section \ref{RQ4answer}).




\begin{table*}[t]
	\centering
	\caption{A working example of BLuAMIR}
	\label{tab:workingexample}
	\resizebox{7.1in}{!}{%
		\begin{tabular}{c|c|c|c||c|c|c|c|c}
			\hline
			%\begin{tabular}[c]{@{}c@{}}\#Bugs for \\ developing \\ map databases\end{tabular} &
			\textbf{Rank}
			& \textbf{Retrieved Files (VSM Approach)} 
			&  \textbf{Score\_{VSM}}  & 
			\textbf{Ground Truth}  &
				\textbf{Retrieved Files (BLuAMIR)} &
			\textbf{Score\_{VSM}}  & 
			\textbf{Score\_{Assoc}} &
			\textbf{Score\_{Total}} &
		
			\textbf{Ground Truth}\\
			\hline
			\hline
			1 &  WorkingSetLabelProvider.java  &1.00  & \checkmark &WorkingSetLabelProvider.java & 0.60  & 0.25  & 0.85 & \checkmark \\
			\hline
			2 & ImageFactory.java  & 0.80  & \xmark & WorkingSet.java & 0.35 & 0.35 & 0.70 &  \checkmark \\ 
			\hline
			3 & WorkingSetMenuContributionItem.java &  0.76  & \xmark &IWorkingSet.java &0.45 &0.22 & 0.67 &  \checkmark\\
			\hline
			4 &  IWorkingSet.java &  0.75 & \checkmark & WorkingSetTypePage.java &0.38 & 0.25 & 0.63 &  \xmark\\
			\hline
			5 & ProjectImageRegistry.java  & 0.70  & \xmark & CommandImageService.java &0.30 &0.33 & 0.63 &  \xmark \\
			\hline
%			& 0.68 & IWorkingSetManagerTest.java & \xmark &0.31 & 0.29 & 0.60 & WorkbenchImages.java & \checkmark\\
%			\cline{2-9}
%			& 0.67 & IWorkbenchPartDescriptor.java & \xmark & 0.33 & 0.27 & 0.60 & WorkingSetAdapterFactory.java & \checkmark \\
%			\cline{2-9}
%			& 0.65 & MockWorkingSetPage.java & \xmark  & 0.30 & 0.27 & 0.57 & EditorIconTest.java & \xmark\\
%			\cline{2-9}
%			& 0.65 & MissingImageDescriptor.java & \xmark  &0.30 &0.27 & 0.57 & PerspectiveDescriptor.java & \xmark \\
%			\cline{2-9}
%			& 0.64 & WorkingSetTypePage.java & \xmark & 0.24 & 0.33 & 0.57 & IAction.java & \xmark\\
		
	\end{tabular}}
	\centering
\end{table*}
\begin{table}[htbp]
	\caption{A Bug Report (\#37026, eclipse.UI.platform)}
	\label{tab:BugInfo}
	%\centering
	\begin{center}
		\begin{tabular}{ p{1.5cm} | p{6cm}}
			\hline
			\textbf{Field}  & \textbf{Content} \\
			\hline
			\hline
			Title & [Working Sets] IWorkingSet.getImage should be  getImageDescriptor
			\\
			\hline
			Description &  build 20030422 \\ &
			IWorkingSet.getImage returns an ImageDescriptor and should therefore be named 
			getImageDescriptor.
			This is new API in 2.1. Should consider deprecating getImage and creating 
			getImageDescriptor.\\
			\hline
		\end{tabular}
	\end{center}
	%\centering
\end{table}


So, contributions of our paper include:
\begin{itemize}
	\item A novel bug localization technique that not only considers lexical similarity but also exploits the association relationship between bug report keywords and their corresponding buggy source code.
	\item Comprehensive evaluation of the technique using total of 3973 bugs from Eclipse, SWT and Zxing bug repository and validation against state-of-the-art bug localization technique \cite{Jian}.
\end{itemize}

The rest of the paper is organized as follows. Section \ref{sec:motivatingexample} discusses a motivating example of our proposed approach, and Section \ref{sec:proposedmethod} presents proposed bug localization method for BLuAMIR, and Section \ref{sec:expANDdiss} focuses on the conducted experiments and experimental results, and Section \ref{sec:threats} identifies the possible threats to validity, and Section \ref{relatedwork}
discusses the existing studies related to our research, and finally,  
Section \ref{sec:conclusionANDfuture} concludes the paper with future plan.
\section{Motivating Example}\label{sec:motivatingexample}
Let us consider a bug report (ID 37026) on the Eclipse-UI-Platform. Table \ref{tab:BugInfo} shows the title and description of the encountered bug report. We preprocess the title as well as the description of this bug report, and construct a query. Now we apply both Vector Space Model (VSM) and our approach on this query, and then locate possible buggy files. 
%The retrieved ranked results are presented in Table \ref{tab:workingexample}. 
%From the commit log, it is found that gold set contains 14 number of source code for this bug. 
In Table \ref{tab:workingexample}, we can see that our proposed tool BLuAMIR correctly identifies 3 buggy files where VSM approach locates only 2 buggy files in Top-5. The ranking from basic VSM is 1 and 4 where the rank from BLuAMIR is 1,2,3. However, VSM and association scores are also provided for each recommended buggy file. 
Note that Same file is retrieved in rank 1 for both techniques. 
The buggy file,  
IWorkingSet.java was in the fourth position by VSM, but BLUAMIR returned it at the second position which is promising.

We also investigate why BLuAMIR can locate more buggy files than VSM.  
VSM score is based on overlap of terms found both in the bug reports and source files. On the contrary, association score dont directly compare terms between bug reports and source files. So vocabulary mismatch problem is resolved here. Moreover, in the working example, this association map aids locating more buggy files than VSM technique. IF we go deeper, we can find that due to vocabulary missmatch problem, VSM failed to retrieve buggy files. Our proposed approach makes use of an association map that successfully captures previous relationship between bug report keywords and their source buggy files. 
In BLuAMIR, we collect keywords from a current bug report (i.e., query) and then retrieve their associated source files from the constructed association map. 
This association map is created from keywords extracted from previously fixed bug report and their fixed buggy codebase. So, the idea is, if a current bug shares same keywords (i.e., concepts) with a previous fixed bug, there is possibility they might share some codebase. This is how our proposed approach overcome vocabulary missmatch problem.

\begin{figure*}
	\centering
	\includegraphics[scale=0.38]{SD5-Gray}
	\caption{Proposed Schematic Diagram: (a)Construction of association map between bug report keywords and source documents and (b) Bug Localization Using VSM and association score}
	\label{fig:systemDiagram}
\end{figure*}
\section{BLUAMIR: Proposed Method for Bug Localization} \label{sec:proposedmethod} 
Figure \ref{fig:systemDiagram} shows the schematic diagram of our proposed approach.
%Our proposed approach combine lexical similarity and co-occurence similarity measure.  \citet{Jian} proposed BugLocator based on two different similarity scores- one is rVSM score and the other one is Simi score. 
First, we (a) create an association mapping where the inherent association between past bug reports and their changed documents are leveraged.
%In BLuAMIR, we follow two different approach for computing source code ranks.
Then we (b) retrieve relevant ranked source code files based on two different scores - VSM and association.  
%However, we have divided our approach into two different sections or parts- 1) calculate rVSM and Simi scores or VSM score and co-occurence measure and 2) combine all three or two scores in order to localize recommended buggy source files for a given newly reported bug. 
%As we have combined two existing scores with our proposed word co-occirence score, 
We discuss different parts of our proposed approach, BLuAMIR in the following sections.

\subsection{Association Mapping Database Construction}\label{sec:MapConstruction}
We construct an association mapping database - between keywords extracted from previously fixed bug reports and source code links. 
%The system diagram for Part I is given in Fig. \ref{fig:systemDiagram}(a).
This mapping construction involves two steps.
First we create association map using information contained in bug reports and commit logs. We collect title and description of each bug report. We extract keywords from each bug report after standard natural language preprocessing (i.e., punctuation removal, stop word removal). 
%In this part, information are collected from bug report, version repository and source code repository. 
From version repository, 
%we have commit logs, where the developers commit for several changes in a software project. For example, when a bug report is fixed, the developer who fixed this bug also creates a commit log containing which type of change he had to made to resolve the bug associated with the location of the source code files where the change has made. So, if we analyze commit logs, 
we retrieve source code files which have been changed in order to fix a given bug. %Here the idea is, we first extract keywords from bug report and corresponding buggy source code files information for the same bug report from commit log. Then
It should be noted each commit establishes a direct relationship between its changed documents and the corresponding bug report. 
Thus, the keywords from this bug report enjoy an implicit relationships with the changed source documents. 
We construct an association map database by leveraging this connected relationship. Mapping each keyword can be linked to one or more source code files according to commit logs. Similarly, each source code file can be linked to one or more keywords. 
Algorithm 1 shows pseudo-code for the construction of our mapping database. 
We divide this into three steps step as follows.  
%This way we connect keywords and source code files to form an association map database named keyword-source code links. 

%This section can be further divided into several parts: keyword extraction from bug reports, source code information extraction from commit logs, keyword- source code linking. We also create an algorithm for creating association map database between bug report keywords into their buggy source files, which is given in Algorithm \ref{algo:map}. However, several steps of map database construction are discussed in the followings:

\begin{algorithm}[!t]
	\caption{Construction of Association Mapping Database}
	\label{algo:map}
	\begin{algorithmic}[1]
		\Procedure{BUG REPORTS }{$BRC$}
		
		\Comment{$BRC$: a collection of bug reports}
		\State $MAP_{KS} \gets$ \{\}
		\Comment{an empty association map}
		%\Comment{creating adjacency map database from the bug reports collection}
		%\State $MAP_{adj} \gets$ createAdjacencyDatabase($BRC$)
		%\Comment{creating a map that links keywords into their bug ids}
		\State $MAP_{bk} \gets$ createBugIDtoKeywordMap()
		
		\Comment{Create Map between BugID and Keywords}
		
		\State $MAP_{kb} \gets$ createKeywordtoBugIDMap()
		
		\Comment{Create Map between Keywords and BugID}
		
		\State $MAP_{bs} \gets$ createBugIDtoSourceMap()
		
		\Comment{Create Map between BugID and codebase}
		
		\Comment{Linking keywords into change source code}
		%\LineComment{preprocess the collected keywords}
		%\For{Keyword $K_i \in$ $K$}
		\State $KB \gets$ collectKeywords($MAP_{kb}$)
		
		
		
		\For{keywords $KB_{_i}$ $\in$ $KB$}
		\State $BUG_{id} \gets$ retrieveBugIds ($KB_{_i}$)
		\For{each bug id $BUG_{id_j} \in BUG_{id}$}
		\State $SF_{link} \gets$ getLinkedSourceFiles($MAP_{bs}$, $BUG_{id}$)
		
		\Comment{Retrieve associated source from $MAP_{bs}}$
		%\State $MAO_{BUG_{id}} \gets$ linkSourceCodeFiles$Adj_{T_i}, Adj_{K_j}$) 
		\State $MAP[KB_i].link \gets MAP[KB_{i}].link + SF_{link}$
		
		\Comment{maps all source code files to its keywords}
		\EndFor
		\EndFor
		
		
		\State $MAP_{KS} \gets$ $MAP[KB]$ 
		
		\Comment{collecting all keyword-source code links}
		%\LineComment{put them into map}
		%\State $MAP \gets$ mapping()
		\State \textbf{return} $MAP_{KS}$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}



\textbf{Keyword Extraction from Bug Reports:} We collect title and description from each fixed bug report from a collection of previously fixed bug reports.. We perform standard natural language pre-processing on the corpus such as stop word removal, splitting and stemming. Stop words contain very little semantic for a sentence. Stemming extracts the root of each of the word. We use this stop word list\footnote{{1}https://www.ranks.nl/stopwords} during stop word removal and Porter Stemming stemmer\footnote{{2}http://tartarus.org/martin/PorterStemmer/} for stemming. First we create a mapping relationship between bug IDs and keywords contained in those bug reports, as described in Algorithm \ref{algo:map} line 3. Second we also construct another connected relationship between keywords and bug IDs, presented in
line 4 of Algorithm \ref{algo:map}.

%{\footnotesize \textsuperscript{1}http://tartarus.org/martin/PorterStemmer/}


\textbf{Source Code Link Extraction from Commit Logs:}
We go through all commit messages and identify those commits that contain keywords related to bug fix such as fix, fixed and so on.
In  GitHub, we  note  that any commit that either solves a software bug or implements a feature request generally mentions the corresponding Issue ID
in the very title of the commit.
We identify such commits from the commit history of each of the selected projects using suitable regular expressions, and select them for our experiments
\cite{Bachmann}. Then, we collect the changeset (i.e., list of changed files)
for each of those commit operations, and develop solution set (i.e.,
goldset) for the corresponding change tasks. Thus, for experiments, we collect not only the actual change requests from the reputed subject systems but also their solutions which were applied in practice by the developers \cite{SHaiduc}. We use several
utility commands such as git, clone and log on GitHub for collecting those information. This step is done line 5 of Algotrithm \ref{algo:map}. 
%Each of these commit messages presents other information such as the ID of bug report for which it was created and the links of the corresponding changed source code files. 
%We then construct a linking relationship between each fixed bug report ID into their changed source code files, which is mentioned in line 4 of Algorithm \ref{algo:map}.

\textbf{Keyword-Source Code Linking:}
At this point, we have pre-processed keywords from each bug report. We also have a implied relationship between bug ID and buggy source code links. We construct the bipartite graph (i.e., discussed bellows) between keywords collected from a bug report and buggy source document links. Here, one or more keywords can be linked to a single buggy source code files links. A source code file can be linked to one or more keywords, which is constructed from lines 6-15 in Algorithm \ref{algo:map}.


\begin{table}[htbp]
	\caption{A Bug Report (\#322401, eclipse.UI.platform)}
	\label{tab:BugInfo2}
	%\centering
	\begin{center}
		\begin{tabular}{ p{1.5cm} | p{6cm}}
			\hline
			\textbf{Field}  & \textbf{Content} \\
			\hline
			\hline Title &
		[LinkedResources] Linked Resources properties page should have a Remove button
			\\ \hline
		   Description &  I20100810-0800
		   
		   Project properties $>$ Resource $>$ Linked Resources:
		   Especially for invalid locations, a Remove button would be handy to remove one or multiple links. \\
			\hline
		\end{tabular}
	\end{center}
	%\centering
\end{table}
\begin{figure}
	\centering
	\includegraphics[scale=0.53]{BGraph2}
	\caption{Bipartite Graph Constructed for Bug \#46669. Here, S1 represents \textit{org.eclipse.ui.internal.ide.IDEWorkbenchMessages.java} and S2 represents \textit{org.eclipse.ui.internal.ide.dialogs.LinkedResourceEditor.java}}
	\label{fig:BipartiteGraph}
\end{figure}

\textbf{A Bipartite Graph Example:}

In mathematics, a bipartite graph is a graph in which the vertices can be put into two separate groups so that the only edges are between those two groups, and there are no edges between vertices within the same group. 
Here, two groups correspond to keywords and codebase. Hence, nether two keywords nor two source files can be connected. There is only valid link is established between a keyword and its corresponding source. 
We construct a bipartite graph for a bug report (ID 322401) on the Eclipse-UI-Platform. Table \ref{tab:BugInfo2} shows the title and description of the encountered bug report. The bipartite graph is presented in Figure \ref{fig:BipartiteGraph}. Note, that ten unique keywords are extracted from the bug report (i.e., from title and description) and are connected through edges with their corresponding change codes (i.e., $S1$ and $S2$ ). 
\subsection{Bug Localization Using VSM and Association Scores}

The system diagram for this part has illustrated in Figure \ref{fig:systemDiagram}(b), which 
%In BLuAMIR, we perform bug localization in two different ways - Approach I 
is based on the combination of VSM and association scores (i.e., Figure \ref{fig:systemDiagram}(b)).
%(b)) and Approach II is based on rVSM, Simi and association scores (i.e., Figure \ref{fig:systemDiagram} (c)). 
We have constructed an association map databases from keywords collected from bug report into its corresponding source code information in Section \ref{sec:MapConstruction}.
For the candidate keyword tokens for the initial query, we exploit association map database (i.e., keyword-source code links) and retrieve the relevant source code files. We use some heuristic functions in order to combine two ranks and recommend buggy relevant source files. 
%However, for simplicity we represent the former approach (i.e., Approach I) in our proposed schematic diagram in Fig. \ref{fig:systemDiagram}(b).
%In this approach, we calculate VSM and Co-occurence ranks in order to recommend buggy source codes for a given newly reported bug. 
%On the other hand,
%in our second approach, we work with three different ranks i.e., rVSM, Simi and Co-occurence. We replicate an existing technique proposed by \citet{Jian} for computing rVSM and Similarity scores. Basically, both VSM and rVSM are TF-IDF based score, which is measured between query and source code files.
%On the other hand, Simi score refers to that fact that if a bug is similar to another bug, then they both tend to relate to same sources. However, we describe both scores in Section \ref{Sec:Localize}.


%\section{Existing Approaches}\label{sec:existing}
%As in our proposed tool, we combine lexical similarity with word co-occurence score, here we discuss TF-IDF based bug localization approach.
%\subsection{Lexical Similarity Based Bug Localization Technique:}
%In this technique, each source code file is ranked based on source code file scores. Source code file contains words those can be also occurred in the bug reports. This is considered as a hint to locate buggy files. 
%%If a new bug is similar to a given previously located bug, then there is a possibility that the source code files located for the past bug can provide useful information in finding buggy files for that new bug.
%For locating a new bug we compute similarity scores for all source code files for a given project. However, we need to focus on some concepts which are required to understand our proposed system. They are described as follows:

%\textbf{Ranking based on Classical Vector Space Mode:}
%The basic idea of a VSM (Vector Space Model) or TF-IDF model is that the weight of a term in a document is increasing with its occurrence frequency in this specific document and decreasing with its occurrence frequency in other documents \cite{Jian}.
%In our proposed approach we have used both classical Vector Space Model (VSM) and revised Vector Space Model (rVSM) proposed by \citet{Jian} in order to index and rank source code files. 
%In classic VSM, \textit{tf} and \textit{idf} are defined as follows:
%\begin{equation}
%tf(t,d)=\frac{f_{td}}{\#terms}, idf(t)=log\frac{\#doc}{n_{t}}
%\end{equation}
%Here \textit{tf} is the term frequency of each unique term \textit{t} in a document \textit{d} and \textit{f\textsubscript{td}} is the number of times term \textit{t} appears in document \textit{d}.
%So the equation of classical VSM model is as follows
%\begin{multline}\label{VSMequation}
%VSMScore(q,d)= cos(q,d) =
%\\
%\frac{1}{\sqrt{\sum_{t\epsilon q}}((\frac{f_{tq}}{\#terms})\times log(\frac{\#docs}{n_{t}}))^{^{2}}}\times 
%\\
%\frac{1}{\sqrt{\sum_{t\epsilon d}((\frac{f_{td}}{\#terms})\times log(\frac{\#docs}{n_{t}}))^{2}}}\times
%\\
%\sum_{t\epsilon q\bigcap d}(\frac{f_{tq}}{\#terms})\times (\frac{f_{td}}{\#terms})\times log(\frac{\#docs}{n_{t}})^{2}
%\end{multline}
%This \textit{VSM} score is calculated for each query bug report \textit{q} against every document \textit{d} in the corpus. However, in the above equation \textit{\#terms} refers to the total number of terms in a corpus, \textit{n\textsubscript{t}} is the number of documents where term \textit{t} occurs.

%\textbf{Ranking based on Revised Vector Space Mode:}
%The main difference between classic VSM and revised VSM is that in case of revised version logarithm variant is used in computing term frequency. The equation for calculating term frequency is:
%\begin{equation}
%tf(t,d)=log(f_{td})+1
%\end{equation}
%So the new equation of revised VSM model is as follows:
%\begin{multline}\label{rVSMequation}
%rVSMScore(q,d)=g(\#term)\times cos(q,d)
%\\
%\frac{1}{1+e^{-N(\#terms))}}\times \frac{1}{\sqrt{\sum_{t\epsilon q}}((logf_{tq}+1)\times log(\frac{\#docs}{n_{t}}))^{^{2}}}\times 
%\\
%\frac{1}{\sqrt{\sum_{t\epsilon d}((log {f_{td}+1})\times log(\frac{\#docs}{n_{t}}))^{2}}}\times
%\\
%\sum_{t\epsilon q\bigcap d}(logf_{tq}+1)\times (logf_{td}+1)\times log(\frac{\#docs}{n_{t}})^{2}
%\end{multline}
%%\end{equation}
%This \textit{rVSM} score is calculated for each query bug report \textit{q} against every document \textit{d} in the corpus. However, in the above equation \textit{\#terms} refers to the total number of terms in a corpus, \textit{n\textsubscript{t}} is the number of documents where term \textit{t} occurs.
%
%\textbf{Ranking based on similar bug information}
%\begin{figure}
%	\centering
%	\includegraphics[scale=0.65]{3layers}
%	\caption{Bug and its Similar Bug Relationship with Source Code Files:}
%	\label{fig:BSBR}
%\end{figure}
%The assumption of this ranking is similar bugs of a given bug tend to modify similar source code files. Here, we construct a 3-layer architecture as described in \cite{Jian}. In the top layer (layer 1) there is a bug \textit{B} which represents a newly reported bug. All previously fixed bug reports which have non-negative similarity with bug  \textit{B} are presented in second layer. In third layer all source code files are shown. In order to resolve each bug in second layer some files in the corpus were modified or changed, which are indicated by a link between layer 2 and layer 3. Foe all source code files in layer 3, similarity score is computed, which can be referred to as degree of similarity. The score can be defined as:
%
%\begin{equation}\label{Simiequation}
%SimiScore=\sum_{All S_{i} that connect to F_{j}}(Similarity(B,S_{i})/n_{i})
%\end{equation} 
%Here, similarity between newly reported bug \textit{B} and previously fixed bug \textit{S\textsubscript{i}} is calculated based on cosine similarity measure and \textit{n\textsubscript{i}} is the total number of link \textit{S\textsubscript{i}} has with source code files in layer 3.

%%\subsection
%\textbf{Combining Both Ranks:}
%We combine the both scores based on source code score and similar bugs score as in \cite{Jian} as follows:
%\begin{equation}
%FinalScore=(1-\alpha )\times N(rVSMScore)+\alpha \times N(SimiScore)
%\end{equation}



%\subsection{LDA Topic Based Bug Localization Technique:}
%The main assumption behind this technique is the textual content of the bug reports and their associated buggy source code files tend to describe some common technical aspects. So, if  we could identify the technical topics extracted from both bug reports and source codes, we could recommend the files shared technical topics with the newly reported bug reports. If a newly reported bug has similar technical topics with some previously fixed bug reports, the fixed files could be a good candidate files for newly reported bug.
%
%LDA (Latent Dirichlet Allocation) is a probabilistic and fully generative topic model. It is used to extract latent (i.e., hidden) topics, which are presented in a collection of documents. It also model each document as a finite mixture over the set of topics [add link here]. In LDA, similarity between a document \textit{d} and a query \textit{q} is computed as the conditional probability of the query given that document \cite{Lukins2}.
%
%\begin{equation}
%Sim(q,d_{i})=P(q|d_{i})=\prod_{q_{k}\epsilon q}P(q_{k}|d_{i})
%\end{equation}
%
%Here \textit{q\textsubscript{k}} is the \textit{k}th word in the query {q} Thus, a document (i.e., source code file) is relevant to a query if it has a high probability of generating the words in the query.
%
%We perform the following steps in order to localize buggy files for newly reported bug using LDA based approach:
%
%\begin{itemize}
%	\item Apply topic modeling on the source code files. The output contains a certain number of topics and some associated keywords for each topic. We also get some other distribution files such as document-topic, word-topic etc.
%	\item Now work with the documents topic distribution file. Make a list of source code documents or files for each topic. So, we wiill have a list that contain all topics and their associated source code documents.
%	\item Here our query is the newly reported bug. This contains information in the bug reports such as title and short description etc. We all do inference for this query using a topic modeling tool. It will extract all topic associated with the query (i.e., newly reported bug).
%	\item Now we need to work with topic keywords. We are going to perform a comparison between newly reported bug or the given query and source code files using topic information. That means we will compare topic-keywords associated with topics inferred for the query with topic-keywords of each topic extracted from source code documents.
%	\item We will rank them based on topic-keyword similarity. So, now we know which are the top most topics, and we already have information regarding topic-document relationship, we will retrieve all source code files associated with all those top most topic as recommended buggy files.
%	%\item A detailed description of methodologies for visualizing topic evolution extracted from bug reports.
%	%\item A detailed description of methodologies for visualizing bug report extractive summaries.
%	%\item Evaluation of visualized bug report extractive summary by conducting a task-oriented user study.
%\end{itemize}


%\section{Part I: Association Map Database Construction}\label{sec:MapConstruction}
%%Our proposed approach consists of two parts - (i) constructing association map databases and (ii) retrieve relevant buggy source code files. 
%
%%\subsection{Part I: Construction of Association Map Database Between Bug Reports and Source Files}
%In this part, we construct an association map databases - between keywords and corresponding source code links extracted from bug reports and commit messages respectively. This section can be further divided into several parts: keyword extraction from bug reports, source code information extraction from commit logs, keyword- source code linking. We also create an algorithm for creating association map database between bug report keywords into their buggy source files, which is given in Algorithm \ref{algo:map}. However, several steps of map database construction are discussed in the followings:





%\section{Part II: Localizing Buggy Source Code Files} \label{Sec:Localize}
%In this part, we combine existing two lexical similarity based bug localization approaches with our proposed keyword-source co-occurence relation. One is with VSM score and other one is with rVSM and Simi ranks which are proposed in BugLocator \cite{Jian}. So, there are two sub parts of this section.
%\subsection{Approach I: Combinition of VSM and Co-occurence Rank:}
For locating a new bug we compute similarity scores for all source code files for a given project. However, we need to focus on some concepts which are required to understand our proposed system. They are described as follows:

\textbf{Vector Space Model:}
The vector space model (VSM) is a classical method for constructing vector representations for documents \cite{Salton}. 
It encodes a document collection by a term-by-document matrix. It represents one type of text unit (documents) by its association with the other type of text unit (terms). Here, the association is calculated by explicit evidence based on term occurrences in the documents.
The similarity between documents is computed by the cosine or inner product between corresponding vectors.

\textbf{VSM Score Calculation:}
Source code file contains words those can be also occurred in the bug reports. This is considered as a hint to locate buggy files. 
%If a new bug is similar to a given previously located bug, then there is a possibility that the source code files located for the past bug can provide useful information in finding buggy files for that new bug.
The basic idea of a VSM (Vector Space Model) is that the similarity between documents is computed by the cosine or inner product between terms collected from a query and a source document.
The weight of a term in a document increases with its occurrence frequency in this specific document and decreases with its occurrence frequency in other documents.
%In our proposed approach we have used both classical Vector Space Model (VSM)  and revised Vector Space Model (rVSM) proposed by \citet{Jian} in order to index and rank source code files. 
In classic VSM, {$tf$} (i.e., term frequency) and {$idf$} (i.e., inverse document frequency) are defined as follows:
\begin{equation}
tf(t,d)=\frac{f_{td}}{\#terms}, idf(t)=log\frac{\#doc}{n_{t}}
\end{equation}
Here $tf$ is the term frequency of each unique term {$t$} in a document {$d$} and {$f\textsubscript{td}$} is the number of times term {$t$} appears in document {$d$}.
So the equation of classical VSM model is as follows
\begin{multline}\label{VSMequation}
VSMScore(q,d)= cos(q,d) =
\\
\frac{1}{\sqrt{\sum_{t\epsilon q}}((\frac{f_{tq}}{\#terms})\times log(\frac{\#docs}{n_{t}}))^{^{2}}}\times 
\\
\frac{1}{\sqrt{\sum_{t\epsilon d}((\frac{f_{td}}{\#terms})\times log(\frac{\#docs}{n_{t}}))^{2}}}\times
\\
\sum_{t\epsilon q\bigcap d}(\frac{f_{tq}}{\#terms})\times (\frac{f_{td}}{\#terms})\times log(\frac{\#docs}{n_{t}})^{2}
\end{multline}
This \textit{VSM} score is calculated for each query bug report {$q$} against every document {$d$} in the corpus. However, in the above equation {$\#terms$} refers to the total number of terms in a corpus, {$n\textsubscript{t}$} is the number of documents where term {$t$} occurs.

\textbf{Association Scores Calculation}
A query typically contains several keywords or words. For each keyword, we look for relevant source files in the keyword-source files association map. We assume these files are relevant because we created the map between the content of bug reports and their buggy source files for previously fixed bug reports. When we analysis these links for all keywords in a query, a relevant file can be found from the association relationship more than once. Therefore, we then normalize the frequency of source files using standard TFIDF normalization technique. Finally we recommend first Top-K files with their association. The equation for computing association score is given belows:
\begin{equation}\label{CoOccequation}
AssociationScore=\sum_{All S_{i} that connect to W_{j}}(Link(W{j},S_{i}))
\end{equation}
Here, the link $Link(W{j},S_{i})$ between keyword and source file is 1 if they are connected in the association map and 0 otherwise.


\textbf{Final Score Calculation}
We compute VSM score using Appache Lucene library. Then we combine that score with our association using equation \ref{equationVSMme}.
\begin{multline}\label{equationVSMme}
FinalScore=(1-\alpha )\times N(VSMScore)+ \\
\alpha \times N(AssociationScore)
\end{multline}
Here, the weighting factor $\alpha$ varies from 0.1 to 0.5, for which we discuss results in the experiment section \ref{RQ2Answer}.





\section{Experiment and Discussion} \label{sec:expANDdiss}
In this section, at first we discuss detail of our data set, then we describe the evaluation metrics, research questions and finally we present our experimental results.




%\begin{table}[htbp]
%\caption{Table Type Styles}
%\begin{center}
%\begin{tabular}{|c|c|c|c|}
%\hline
%\textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
%\cline{2-4} 
%\textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
%\hline
%copy& More table copy$^{\mathrm{a}}$& &  \\
%\hline
%\multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
%\end{tabular}
%\label{tab1}
%\end{center}
%\end{table}



\subsection{Experimental Dataset}
We work with three different dataset - Eclipse, SWT aand Zxing.
We work with Eclipse data set which is a popular IDE for Java. We downloaed a git based Eclipse project from git repository \footnote{https://git.eclipse.org/c/platform/eclipse.platform.ui.git/}. We work with Eclipse Platform UI project. 
%These source codes are contained in our source code repository. 
On the other hand, currently Eclipse Platform UI project contains more than 10K number of bugs where we only work with the bugs which are fixed. We create quires from each bugs considering their title and short summary.
%We have two parts in our corpus. One is source code files downloaded as git based project and another part is bug reports collection. 
All bug reports are collected from \footnote{https://bugs.eclipse.org/}. In order to obtain the links between previously fixed bugs and source code files, we analyze git project commit message. We ran through all commit messages and track Bug IDs associated with examined source code files. 
In order to evaluate our proposed tool we have also used two more dataset that \citet{Jian} used
to evaluate BugLocator. This dataset contains 118 bug reports in total from two popular open source projects– SWT, and ZXing along with the information of fixed files for those bugs. The detail of our dataset is presented in \ref{tab:DDSl}. SWT is a component of Eclipse \footnote{http://www.eclipse.org/swt/}. Zxing is an android based project maintained by google \footnote{http://code.google.com/p/zxing/}.
\begin{table}[htbp]
	\caption{Description of Data Sets}
	\label{tab:DDSl}
	%\centering
	\begin{center}
		\begin{tabular}{ p{1cm} | p{2.5cm} | p{1.5cm} | p{.75cm} | p{.75cm} }
			\hline
			\textbf{Project Name}  & \textbf{Description} & \textbf{Study Period}& \textbf{\#Fixed Bugs} & \textbf{\#Source Files}\\
			\hline
			{Eclipse Platform Ant} & Popular IDE for Java & Nov 2001 - April 2010 & {3855} & 11732\\ \hline
			SWT (V 3.1)& An open source widget toolkit for Java & Oct 2004 - Apr 2010 & 98 & 484 \\ \hline
			Zxing & A barcode image processing library for Android Application & Mar 2010 - Sep 2010 & 20 & 391 \\
			\hline
			
		\end{tabular}
	\end{center}
	%\centering
\end{table}


\subsection{Evaluation Metrices}
To measure the effectiveness of the proposed bug localization approach, we use the following metrics:

\textbf{Ton N-Rank (Hit@N):} It represents the number of bug, for which their associated files are returned in a ranked list. Here, \textit{N} may be 1, 5 or 10. We assume that if at least one associated file is presented in the resulted ranked list, then the given bug is located. The higher the metric value, the better the bug localization performance

\textbf{MRR(Mean Reciprocal Rank)}
The reciprocal rank of a query is the multiplicative inverse of the rank of the first correct answer. So mean reciprocal rank is the average of the reciprocal ranks of results of a set of queries $Q$
\begin{equation}
MRR=\frac{1}{\left | Q \right |}\sum_{i=1}^{\left | Q \right |}\frac{1}{rank_{i}}
\end{equation}
where $ranki$ is the position of the first buggy file in the returned ranked files for the first query in $Q$.

\textbf{MAP(Mean Average Precision)}
Mean Average Precision is the most commonly used IR metric to evaluate ranking approaches. It considers the ranks of all buggy files into consideration. So, MAP emphasizes all of the buggy files instead of only the first one.
MAP for a set of queries is the mean of the average precision scores for each query. The average precision of a single query is computed as:
\begin{equation}
AP=\sum_{k=1}^{M}\frac{P(k)\times pos(k)}{number \, of \, positive \, instances}
\end{equation}
where $k$ is a rank in the returned ranked files, $M$ is the number of ranked files and $pos(k)$ indicates whether the kth file is a buggy file or not. $P(k)$ is the precision at a given top $k$ files and is computed as follows:
\begin{equation}
P(k)=\frac{\#buggy \, Files}{k}
\end{equation}

%\textbf{Wilcoxon signed-rank test}
%The Wilcoxon signed-rank test is a non-parametric statistical hypothesis test used to compare two related samples, matched samples, or repeated measurements on a single sample to assess whether their population mean ranks differ. We perform this test with the help of \footnote{https://www.socscistatistics.com/tests/signedranks/Default.aspx}.
%
%\textbf{Cross Validation}
%We divide our query data into k number of sets. Typically k is 10, but we work with k =5 and k=10. Each set contains a training set and tesing set. Training data is used to create mapping between keywords extracted from bug reports and source code files. 10-fold-cross validation data is presented in table \ref{tab:Performance1} and table \ref{tab:Performance2}.


\subsection{Research Questions}
Our proposed tool-BLuAMIR are designed to  answer the following research questions.
\begin{itemize}
	\item RQ1: How many bugs can be successfully located by BLuAMIR?
	\item RQ2: Does our proposed approach-BLuAMIR resolve the vocabulary missmatch problem and how?
	\item RQ3: Does our proposed approach-BLuAMIR eliminate large files problem and how?
	\item RQ4: In BLuAMIR comparable with the state-of-the-art techniques in identifying buggy files?
\end{itemize}






\subsection{Experimental Results}
During experiment, we evaluate our proposed approach in different ways. To create mapping between bug report keywords and source files, we consider three different options - (1) including only title or summary of a bug report in creating corpus, (2) in addition with title we also include description field of a bug report and (3) full content of a bug report could be an option. Neither option 1 nor 3 provides better result and option 2 optimized the performance. We explain this in a way that providing only title of a big report conveys very little information. On the other hand, including full content of a bug report also create too much information that contains huge noise data and also takes longer time during mapping them into source code files. Therefore, title and description of a bug report optimized those two options. However, considering title and description did not get rid of noise and therefore we discard all keywords that happen to exist in 25\% or more documents in the corpus.

%However, we divide our experimental result into two parts based on experimented dataset. 
%We implemented BLuAMIR by two different approaches. 
We work on Eclipse dataset, where we compare the performance of BLuAMIR with replicated BugLocator \cite{Jian} and VSM based bug localization techniques. The detail of this comparison is presented in the following subsection.
%For that, we replicate BugLocator proposed by \citet{Jian}.
On the other hand, we also experiment with SWT and Zxing dataset as in \cite{Jian}. Here we collect the results reported in \cite{Jian} and then compare the performance with BLuAMIR. We also answer our research questions in the following subsections.

\subsection{Comparison Between Replicated BugLocator and BLuAMIR}
%\subsection{Bug Localization using rVSM, Simi and Association Scores:}
First, we replicate an existing technique (i.e., BugLocator) proposed by \citet{Jian}. 
\citet{Jian} uses
revised Vector Space Model (rVSM) and Simi scores in order to index and rank source code files.
Second, we combine our proposed association score with these two scores ranks (i.e., rVSM, Simi). Finally, we perform comparison between replicated BugLocator with this combined scores in terms of Top-1, Top-5, Top-10, MRR and MAP. Ranking based on rVSM and Simi and our combined approach are depicted as follows.
%If a new bug is similar to a given previously located bug, then there is a possibility that the source code files located for the past bug can provide useful information in finding buggy files for that new bug.
%On the other hand, Simi score refers to that fact that if a bug is similar to another bug, then they both tend to relate to same sources. However, we describe both scores in Section \ref{Sec:Localize}

\textbf{Ranking based on Revised Vector Space Mode:}
The main difference between classic VSM and revised VSM is that in case of revised version logarithm variant is used in computing term frequency. The equation \cite{Jian} for calculating term frequency is:
\begin{equation}
tf(t,d)=log(f_{td})+1
\end{equation}
So the new equation of revised VSM model is as follows:
\begin{multline}\label{rVSMequation}
rVSMScore(q,d)=g(\#term)\times cos(q,d)
\\
\frac{1}{1+e^{-N(\#terms))}}\times \frac{1}{\sqrt{\sum_{t\epsilon q}}((logf_{tq}+1)\times log(\frac{\#docs}{n_{t}}))^{^{2}}}\times 
\\
\frac{1}{\sqrt{\sum_{t\epsilon d}((log {f_{td}+1})\times log(\frac{\#docs}{n_{t}}))^{2}}}\times
\\
\sum_{t\epsilon q\bigcap d}(logf_{tq}+1)\times (logf_{td}+1)\times log(\frac{\#docs}{n_{t}})^{2}
\end{multline}
%\end{equation}
This \textit{rVSM} score is calculated for each query bug report {$q$} against every document {$d$} in the corpus. However, in the above equation {$\#terms$} refers to the total number of terms in a corpus, {$n\textsubscript{t}$} is the number of documents where term {$t$} occurs.

\textbf{Ranking based on similar bug information}
\begin{figure}
	\centering
	\includegraphics[scale=0.48]{3layers-Gray}
	\caption{Bug and its Similar Bug Relationship with Source Code Files:}
	\label{fig:BSBR}
\end{figure}
The assumption is if a new bug is similar to a given previously located bug, then there is a possibility that the source code files located for the past bug can provide useful information in finding buggy files for that new bug.. Here, we construct a 3-layer architecture as described in \cite{Jian}. In the top layer (layer 1) there is a bug \textit{B} which represents a newly reported bug. All previously fixed bug reports which have non-negative similarity with bug  \textit{B} are presented in second layer. In third layer all source code files are shown. In order to resolve each bug in second layer some files in the corpus were modified or changed, which are indicated by a link between layer 2 and layer 3. Foe all source code files in layer 3, similarity score is computed, which can be referred to as degree of similarity. The score can be defined as:

\begin{equation}\label{Simiequation}
SimiScore=\sum_{All S_{i} that connect to F_{j}}(Similarity(B,S_{i})/n_{i})
\end{equation} 
Here, similarity between newly reported bug {$B$} and previously fixed bug {$S\textsubscript{i}$} is calculated based on cosine similarity measure and {$n\textsubscript{i}$} is the total number of link {$S\textsubscript{i}$} has with source code files in layer 3.

\textbf{Final Score Calculation}
For each query, we compute the rVSM score against all source codes in the database using equation \ref{rVSMequation} and we also calculate Simi score using equation \ref{Simiequation}. Then we calculate association scores for the query using equation \ref{CoOccequation}.
We finally combine the three ranks and for that we use three weighting factor {$\alpha$}, $\beta$ and $\gamma$.
The final equation is given in equation \ref{equationBLme}.
\begin{multline}\label{equationBLme}
CombinedScore=\gamma \times N(rVSMScore)+
\\ \beta \times N(SimiScore) + \alpha \times N(AssociationScore)
\end{multline}
We work with different values of $\alpha$, which are presented in the experiment section \ref{RQ2Answer}. We use value of 0.2 for $\beta$, varying $\alpha$ from 0.1 to 0.5 and thus, $\gamma$ from 0.7 to 0.3. So that they end up into 1.

\subsection{Answering RQ1} \label{RQ1answer}
To answer RQ1, we compare the performance of our proposed bug localization approach with two existing techniques - 1) BugLocaotor \cite{Jian} which is based on rVSM and Simi scores and 2) VSM which is based on vector space model. 


\begin{table}[htbp]
	\centering
	\caption{Performance of proposed technique (VSM+Association) Ranks}
	\label{tab:Performance2}
	\resizebox{3.5in}{!}{%
		\begin{tabular}{c|c|c|c|c|c|c}
			\hline
			%\begin{tabular}[c]{@{}c@{}}\#Bugs for \\ developing \\ map databases\end{tabular} &
			
			\begin{tabular}[c]{@{}c@{}}\# \textbf{Test}  \\ \textbf{Case} \\ \end{tabular} & 
			\begin{tabular}[c]{@{}c@{}}\#\textbf{Methodology} \\  \end{tabular} 
			& 
			\begin{tabular}[c]{@{}c@{}}\textbf{Top 1}\\ \%\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{Top 5}\\ \%\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{Top 10}\\ \%\end{tabular} & 
			\textbf{MRR@10} 
			& \textbf{MAP@10} \\ \hline \hline
			\multirow{2}{*}{1}& VSM & 23.67& 46.59&56.97& 0.33 & 0.32 \\ \cline{2-7}
			& VSM + Association & 28.40                                               & 51.77                                            & 60.06                                                &   0.38  & 0.36 \\ \hline
			\multirow{2}{*}{2}                                                                               & VSM & 24.62 & 48.05 & 57.96 & 0.34 & 0.34 \\  \cline{2-7}  &VSM + Association                                                                    & 27.63                                               & 53.15                                              & 61.26                                             &   0.38  &   0.37  \\ \hline
			\multirow{2}{*}{3}                                                                               & VSM & 20.78 & 40.96 & 51.20 & 0.30 & 0.29 \\  \cline{2-7}   &VSM + Association                                                                      & 23.12                                            & 46.85                                            & 59.76                                             &   0.34  &  0.32   \\ \hline
			\multirow{2}{*}{4}                                                                               & VSM & 22.52 & 42.94 & 53.75 & 0.32 & 0.31 \\   \cline{2-7} &VSM + Association                                                                    & 23.42                       & 49.25                       & 61.56                                                &  0.35   &  0.33  \\  \hline
			\multirow{2}{*}{5}                                                                               & VSM & 27.33 & 52.25 & 63.36 & 0.38 & 0.35 \\   \cline{2-7} &VSM + Association                                                                   & 29.73                                                 & 53.15                                                 & 62.16                                                  &  0.39   & 0.36     \\  \hline
			\multirow{2}{*}{6}                                                                               & VSM & 25.00 & 50.60 & 60.84 & 0.36 & 0.34 \\  \cline{2-7}  &VSM + Association
			&26.13 &
			51.65 &
			62.76 & 0.37 &
			0.35     \\  \hline 
			\multirow{2}{*}{7}                                                                               & VSM & 29.82 & 54.52 & 66.27 & 0.40 & 0.38 \\  \cline{2-7}  &VSM + Association
			
			&35.43 &
			60.96 &
			72.07 & 0.46 &
			0.44     \\  \hline
			\multirow{2}{*}{8}                                                                               & VSM & 27.79 & 51.36 & 60.73 & 0.38 & 0.36 \\  \cline{2-7}  &VSM + Association
			&36.64 &
			58.86 &
			67.87 & 0.46 &
			0.43    \\  \hline
			\multirow{2}{*}{9}                                                                               & VSM & 29.13 & 52.55 & 64.86 & 0.39 & 0.36 \\  \cline{2-7}   &VSM + Association
			&29.13 &
			61.86 &
			69.97 & 0.42 &
			0.40    \\  \hline
			\multirow{2}{*}{10}                                                                               & VSM & 19.03 & 39.58 & 51.34 & 0.28 & 0.26 \\  \cline{2-7}  &VSM + Association
			&24.62 &
			51.05 &
			63.06 & 0.36 &
			0.34    \\ \hline \hline
			\multirow{2}{*}{Average}                                                                               & VSM & 24.98\% & 47.94\% & 58.73\% & 0.35 & 0.33 \\  \cline{2-7}   &VSM + Association     & 28.43\%                                                 & 53.86\%                                                 & 64.05\%                                                  &   0.39  &  0.37    \\ 
			\hline
	\end{tabular}}
	\centering
\end{table}
\textbf{VSM vs Our proposed Tool}
We combine VSM score and association scores in order to produce ranked result. We also compare the performance of baseline VSM technique and our proposed combined approach. The comparison is presented in table \ref{tab:Performance2}. We compute Top-1, Top-5, Top-10 performance and MRR and MAP for both approaches. In all cases our proposed approach outperforms VSM-based bug localization approach. The Top-10 performance of our tool is 64.05\% whereas it is 58.73\% for VSM.
So. to answer our \textit{RQ1}, we can go to Table \ref{tab:Performance2}, 64.05\% bugs are successfully located in Top-10 for Eclipse dataset for Approach I of BLuAMIR. 

We also compute Wilcoxon signed-rank test both for MRR and MAP. For MRR the {Z} -value is -2.8031. The {p} -value is 0.00512. The result is significant at p<=0.05. The W-value is 0. The critical value of W for N = 10 at p<=0.05 is 8. Therefore, the result is significant at p<=0.05.
For MAP - the {Z} -value is -2.8031. The {p} -value is 0.00512. The result is significant at p<=0.05. The W-value is 0. The critical value of W for N = 10 at p<=0.05 is 8. Therefore, the result is significant at p<=0.05.


\begin{table}[htbp]
	\caption{Performance of Bugloactor and proposed technique (rVSM+Simi+Association)}
	\label{tab:Performance1}
	\centering
	\resizebox{3.4in}{!}{%
		\begin{tabular}{c|c|c|c|c|c|c}
			\hline
			%\begin{tabular}[c]{@{}c@{}}\#Bugs for \\ developing \\ map databases\end{tabular} &
			\begin{tabular}[c]{@{}c@{}}\# \textbf{Test}  \\ \textbf{Case} \\ \end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{Methodology} \\ \end{tabular} &
			%\textbf{alpha} &
			%\textbf{beta}&
			%\textbf{gamma}& 
			\begin{tabular}[c]{@{}c@{}}\textbf{Top 1}\\ \%\end{tabular} & 
			\begin{tabular}[c]{@{}c@{}}\textbf{Top 5}\\ \%\end{tabular} & 
			\begin{tabular}[c]{@{}c@{}}\textbf{Top 10}\\ \%\end{tabular} &
			
			\begin{tabular}[c]{@{}c@{}} \textbf{MRR} \end{tabular} & 
			\begin{tabular}[c]{@{}c@{}} \textbf{MAP} \end{tabular} \\ \hline \hline
			\multirow{2}{*}{1}  &Replicated BugLocator     &  8.88& 24.56&34.32& 0.16 & 0.15 \\  \cline{2-7}
			& rVSM+Simi+ Association                                                                                                                                           & 16.27                                               & 39.35                                            & 49.41                                                &   0.26  & 0.25    \\ \hline
			\multirow{2}{*}{2}  &Replicated BugLocator     &  9.9& 24.62&34.53& 0.17 & 0.16 \\ \cline{2-7}
			& rVSM+Simi+ Association                                                                 & 18.32                                               & 41.44                                              & 53.45                                             &   0.28  &   0.27  \\ \hline
			\multirow{2}{*}{3}  &Replicated BugLocator     &  7.50& 21.32&30.63& 0.14 & 0.13 \\ \cline{2-7}
			& rVSM+Simi+ Association                                                                   & 18.62                                            & 42.34                                            & 52.25                                             &   0.28  &  0.27   \\  \hline
			\multirow{2}{*}{4}  &Replicated BugLocator     &  8.1& 21.02&29.13& 0.14 & 0.13 \\  \cline{2-7}
			& rVSM+Simi+ Association                                                                   & 14.71                       & 42.04                       & 54.35                                               &  0.26   &  0.25  \\  \hline
			\multirow{2}{*}{5}  &Replicated BugLocator     &  9.6& 30.63&42.94& 0.18 & 0.18 \\  \cline{2-7}
			& rVSM+Simi+ Association                                                                 & 22.22                                                 & 42.34                                                & 57.66                                                  &  0.31   & 0.30     \\ \hline
			\multirow{2}{*}{6}  &Replicated BugLocator     &  10.21& 31.53&43.54& 0.19 & 0.19 \\  \cline{2-7}
			& rVSM+Simi+ Association
			&25.53 &
			52.25 &
			60.66 & 0.36 &
			0.34     \\  \hline
			\multirow{2}{*}{7}  &Replicated BugLocator     &  9.61& 30.33&40.84& 0.18 & 0.17 \\ \cline{2-7}
			& rVSM+Simi+ Association
			&25.22 &
			52.25 &
			64.56 & 0.36 &
			0.34     \\  \hline
			\multirow{2}{*}{8}  &Replicated BugLocator     &  8.4& 26.13&39.94& 0.19 & 0.16 \\ \cline{2-7}
			& rVSM+Simi+ Association
			&24.92 &
			51.95 &
			62.46 & 0.36 &
			0.34    \\  \hline
			\multirow{2}{*}{9}  &Replicated BugLocator     &  11.11& 28.83&40.24& 0.19 & 0.18 \\  \cline{2-7}
			& rVSM+Simi+ Association
			&21.92 &
			48.65 &
			62.46 & 0.33 &
			0.32    \\  \hline
			\multirow{2}{*}{10}  &Replicated BugLocator     &  6.6& 20.72&27.93& 0.12 & 0.12 \\  \cline{2-7}
			& rVSM+Simi+ Association
			&16.21 &
			40.54 &
			55.55 & 0.27 &
			0.26    \\  \hline \hline
			\multirow{2}{*}{Average}       &Replicated BugLocator     &  8.99\%& 25.87\%&36.40\%& 0.16 & 0.16  \\ \cline{2-7}
			& rVSM+Simi+ Association                                                                                                                       & 20.39\%                                                 & 45.32\%                                                 & 57.28\%                                                  &   0.31  &  0.29    \\ 
			\hline
	\end{tabular}}
	\centering
\end{table}
\textbf{Replicated BugLocator VS Our Proposed Tool:}
We combine rVSM and simi ranks with our association rank. Here, association rank is computed based on keyword-source code mapping database.In table~\ref{tab:Performance1} we, compare the performance of our proposed approach in terms of top 1, 5, 10 rank, MRR and MAP. We can see that our proposed approach outperforms in all cases. For example, our Top-10 performance 52.94\% has an improvement than BugLocator (36.40\%). This also answers our RQ1.

We also compute Wilcoxon signed-rank test both for MRR and MAP. For MRR the {Z} -value is -2.8031. The {p} -value is 0.00512. The result is significant at p<=0.05. The W-value is 0. The critical value of W for N = 10 at p<=0.05 is 8. Therefore, the result is significant at p<=0.05.
For MAP - the {Z} -value is -2.8031. The {p} -value is 0.00512. The result is significant at p<=0.05. The W-value is 0. The critical value of W for N = 10 at p<=0.05 is 8. Therefore, the result is significant at p<=0.05.
\subsection{Answering RQ4}\label{RQ4answer}
To answer RQ4, we compare the performance of BLuAMIR with BugLocator for the the same dataset for SWT and Zxing presented in Table \ref{tab:performance3}. Here, the results from buglocator is directly copied from their paper \cite{Jian}. We also collect the same bug reports and the same source code repository for both of them. So the results can be compared. For SWT, we can see our tool BLuAMIR performs better for Top-1, Top-5, MRR and MAP. However, for Top-10 BLuAmir is comparable with Buglocator. On the other hand, for Zxing our tool BLuAMIR outperforms for top-5, top-10 and MAP. So we can say that our proposed tool BLuAMIR outperforms most cases and comparable for a few cases with state-of-the-art bug localization technique.
\begin{table}[htbp]
	\centering
	\caption{Performance Comparison between BugLocator and BLuAMIR}
	\label{tab:performance3}
	\resizebox{3.6in}{!}{%
		\begin{tabular}{c|c|c|c|c|c|c}
			\hline
			%\begin{tabular}[c]{@{}c@{}}\#Bugs for \\ developing \\ map databases\end{tabular} &
			
			\begin{tabular}[c]{@{}c@{}}\# \textbf{System}   \\ \end{tabular} & 
			\begin{tabular}[c]{@{}c@{}}\#\textbf{Localization} \\ \textbf{Approach} \\ \end{tabular} 
			& 
			\begin{tabular}[c]{@{}c@{}}\textbf{Top 1}\\ \%\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{Top 5}\\ \%\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{Top 10}\\ \%\end{tabular} & 
			\textbf{MRR@10} 
			& \textbf{MAP@10} \\ \hline \hline
			
			\multirow{2}{*}{SWT}                                                                               & BugLocator & 39.80 & 67.35 & 81.63 & 0.53 & 0.45 \\  \cline{2-7}  &BLuAMIR                                                                     & 44.90                                               & 73.45                                              & 80.61                                             &   0.57  &   0.54  \\ \hline
			\multirow{2}{*}{Zxing}                                                                               & BugLocator & 40.00 & 60.00 & 70.00 & 0.50 & 0.44 \\  \cline{2-7}   &BLuAMIR                                                                  & 30.00                                            & 70.00                                           & 75.00                                             &   0.48  &  0.46   \\ \hline
			
			
			
			\hline
	\end{tabular}}
	\centering
\end{table}

%\textbf{The Ranking Comparison Between Buglocator and BLuAMIR for SWT dataset}
%We also perform a query wise comparison for SWT, which is presented in Fig (link).

\subsection{Answering RQ2}\label{RQ2Answer}
To answer RQ2, we investigate several weighting functions for both of our proposed approaches, which are described as follows:

\textbf{Weighting Function for VSM+Co-ocuurence Ranking (Approach I):}
For our Approach I, we also compute performance TopK accuracy, MRR and MAP for different weighting function such as  \textit{ALPHA} is 0.2, 0.3, 0.4. The results are presented in Table \ref{tab:alphaApproach1}. Here, it shows, more $\alpha$ produces better performance. That means if we increase the association scores with higher weighting function, the better performance is resulted in this proposed approach. 
Adding association score increases the performance in this case also indicate that the association mapping is helping in locating buggy files. Therefore, vocabulary miss-match problem is resolved here (i.e., answering RQ2). 
We also illustrate the impact of $\alpha$ for Top-1, Top-5 and Top-10 retrieval on Eclipse dataset for approach I. (VSM+Co-ocuurence Ranking) in Figure \ref{fig:PerformanceTopK2}.
\begin{table}[htbp]
	\centering
	\caption{Performance of (VSM+Association) for different weighting factors}
	\label{tab:alphaApproach1}
	\resizebox{3.4in}{!}{%
		\begin{tabular}{c|l|c|c|c|c|c}
			\hline
			%\begin{tabular}[c]{@{}c@{}}\#Bugs for \\ developing \\ map databases\end{tabular} &
			{$\alpha$} 
			
			& \textbf{Top 1} \% 
			& \textbf{Top 5} \% & 
			\textbf{Top 10} \% &
			\textbf{MRR}  & 
			\textbf{MAP}  \\
			\hline
			{0.2} 
			& 26.72                                            & 51.94                                            & 62.43                                                &   0.37  & 0.36    \\ \hline
			{0.3} 
			& 28.06                                               & 53.35                                              & 63.24                                             &   0.39  &   0.37  \\ 
			\hline
			{0.4} 
			
			&28.43 &
			53.86 &
			64.05 & 0.39 &
			0.37     \\  
			\hline
			{Average}      
			& 27.74\%                                                 & 53.05\%                                                 & 63.24\%                                                  &   0.38  &  0.37    \\ 
			\hline
	\end{tabular}}
	\centering
\end{table}

\textbf{Weighting Function for rVSM+Simi+Association Ranking (Approach II):}
We compute performance TopK accuracy, MRR and MAP for different weighting function such as  $\alpha$ is 0.2, 0.3, 0.4, $\beta$ is 0.2 nad $\gamma$ is 0.6, 0.5, 0.4 respectively. The results are presented in Table \ref{tab:alphaApproach2}. 
 Here, it shows, $\alpha$ produces better performance. That means if we increase the association scores with higher weighting function, the better performance is resulted. This also prove our association rank is effective in producing better results.
 Adding association score increases the performance also indicate that the association map is helping in locating buggy files. Therefore, vocabulary miss-match problem is resolved here (i.e., answering RQ2). 
 We also represent the impact of $\alpha$ for Top-1, Top-5 and Top-10 retrieval on Eclipse dataset for approach 1 (rVSM+Simi+Association Ranking) in figure \ref{fig:PerformanceTopK}.
\begin{table}[htbp]
\centering
\caption{Performance of (rVSM+Simi+Association) for different weighting factors}
\label{tab:alphaApproach2}
\resizebox{3.4in}{!}{%
\begin{tabular}{c|c|c|c|c|c|c|c}
			\hline
			%\begin{tabular}[c]{@{}c@{}}\#Bugs for \\ developing \\ map databases\end{tabular} &
%			\begin{tabular}[c]{@{}c@{}} \textit{alpha} \end{tabular} & \begin{tabular}[c]{@{}c@{}}Bug\\Localization \\ Technique\end{tabular} & \begin{tabular}[c]{@{}c@{}}Top 1\\ \%\end{tabular} & 
%			\begin{tabular}[c]{@{}c@{}}Top 5\\ \%\end{tabular} & 
%			\begin{tabular}[c]{@{}c@{}}Top 10\\ \%\end{tabular} &
%			\begin{tabular}[c]{@{}c@{}} MRR \end{tabular} & 
%			\begin{tabular}[c]{@{}c@{}} MAP \end{tabular} \\
				{$\alpha$} 
				 & {$\beta$}
				 	 & $\gamma$
				  & \textbf{Top 1} \% 
				  & \textbf{Top 5} \% & 
			\textbf{Top 10} \% &
			 \textbf{MRR}  & 
			 \textbf{MAP}  \\
			 \hline
			{0.2} 
			& 0.2 &0.6                                                                                                                                               & 16.13                                               & 40.40                                            & 52.99                                                &   0.26  & 0.25    \\ \hline
			{0.3} 
			& 0.2  &0.5                                                                    & 18.26                                               & 42.83                                              & 55.73                                             &   0.29  &   0.27  \\ 
			\hline
			
			{0.4}  & 0.2
			& 0.4
			&20.40 &
			45.32 &
			57.28 & 0.31 &
			0.29     \\  
			\hline
			{Average}      
			&                                                                              &                                           & 18.26\%                                                 & 42.85\%                                                 & 55.33\%                                                  &     0.29 &  0.27    \\ 
			\hline
\end{tabular}}
\centering
\end{table}
\begin{figure}
	\centering
	\includegraphics[scale=0.80]{Performance-rvsm+simi+co}
	\caption{The impact of $\alpha$ on bug localization performance (Top-1, Top-5, Top-10)} for proposed approach1.
	\label{fig:PerformanceTopK}
\end{figure}


\begin{figure}
	\centering
	\includegraphics[scale=0.80]{Performance-vsm+co}
	\caption{The impact of $\alpha$ on bug localization performance (Top-1, Top-5, Top-10)} for proposed approach2.
	\label{fig:PerformanceTopK2}
	\end{figure}

\begin{figure}
	\centering
	\includegraphics[scale=0.80]{MRR-SWT-Zxing}
	\caption{The impact of $\alpha$ on bug localization performance (MRR)}
	\label{fig:MRR}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[scale=0.80]{MAP-SWT-Zxing}
	\caption{The impact of $\alpha$ on bug localization performance (MAP)}
	\label{fig:MAP}
\end{figure}
%\subsection{Impact of varying the value of alpha on BLuAMIR in terms of MAP and MRR}

We also evaluate the impact of association score
on bug localization performance, with different $\alpha$ values in terms of MAP and MRR for SWT and Zxing. At the beginning, the bug localization performance increases when the $\alpha$ value increases. However, after a certain point, further increase of the b value will decrease the performance. For example, Figure \ref{fig:MRR} and \ref{fig:MAP} show the bug localization performance (measured in terms of MRR and MAP) for the SWT and Zxing projects. When the $\alpha$ value increases from 0.1 to 0.4, both MRR and MAP values increases. Increasing $\alpha$ value further from 0.4 to 0.7 however leads to lower performance. Note that we obtain the best bug localization performance when $\alpha$ is between 0.3 and 0.4. As association score is based on the association map between keywords and source files, thus no direct matching of vocabulary is required. Therefore, the results obtained from the impact of $\alpha$ on bug localization performance (i.e., MAP and MRR) also suggest the vocabulary missmatch problem is resolved here.

\begin{figure}
	\centering
	\includegraphics[scale=0.45]{RankImprovement}
	\caption{Quey wise comparison between VSM and BLuAMIR for 30 Eclipse Query}
	\label{fig:RankImprovement}
\end{figure}



%We combine co-occurence rank with VSM score in Approach1 and rVSM and Simi scores in Approach2. We apply both of these approaches on Eclipse dataset. We also replicate baseline techniques for those results, which are provided in Table \ref{tab:Performance1} and Table \ref{tab:Performance2}. Here, we can see that both cases BLuAMIR outperforms in terms of Top-1, Top-5, Top-10, MRR and MAP. 


\textbf{Answering RQ3:}
We investigate how large files problem is eliminated in BLuAMIR. We perform a query wise ranking comparison for Approach1 for 30 query from Eclipse system, which is given in Figure \ref{fig:RankImprovement}. Here, X-axis represents query number and Y-axis represents the difference of best rank retried by VSM annd our proposed BLuAMIR. Among 25 query, 10 cases BLuAMIR performs better than VSM, 6 cases VSM retrieves better ranked results than BLuAMIR and 9 cases they both do the same raking retrieval. As most cases BLuMIR provides better ranked results, we closely investigate the ranked results for several bugs. 

\textbf{case \#1} Consider bug \# 138283. The title of this bug is \textit{[KeyBindings] Action set with accelerator causes conflict}. We have found 1 source code file (i.e., org.eclipse.ui.internal.WorkbenchPage.java) in rank \#4 from the obtained results collected from BLuAMIR. No gold set files are resulted from VSM technique in Top-10. However, this source code file is obtained as rank \# 20 in the ranked results collected by applying VSM approach. So, we go deeper into ranking score level. The VSM score for this file is 0.4935, which is pretty low than other files. Because of the length of this file is too large (i.e., contains more than 14K words) making the VSM score too low. On the other hand, in BLuAMIR, this file has Co-Occ score 0f 1.00, which make total score of 0.69, put this on rank \#4. From this case study, it is clear that association mapping between fixed bug report keywords into their corresponding source files can overcome the noise associated with large files. Even if the query and recommended buggy files do not share same keyword but previously shared same concept can aid locating that query. 

%***Find some queries having vocabulary miss-match problem

%\textbf{Answering RQ4:}
%We compare the performance of BLuAMIR with state-of-the-art bug localization technique, BugLocator proposed by \citet{Jian} on two dataset i.e., SWT and Zxing. These results can be found in Table \ref{tab:performance3}. For SWT, we can see our tool BLuAMIR performs better for Top-1, Top-5, MRR and MAP. However, for Top-10 BLuAmir is comparable with Buglocator. On the other hand, for Zxing our tool BLuAMIR outperforms for top-5, top-10 and MAP. So we can say that our proposed tool BLuAMIR outperforms most cases and comparable for a few cases with state-of-the-art bug localization technique.

\section{Threats To Validity}\label{sec:threats}
This section discusses the validity and generalizability of our findings. In particular, we discuss Construct Validity, Internal Validity, and External Validity.

\textbf{Internal Validity:} We used three artifacts of a software repository: bug Reports, source codes and version logs, which are generally well understood. Our evaluation uses three dataset - two of them collected from the same benchmark dataset of bug reports and source code shared by \citet{Jian}, and other is collected from an open source project. Bug reports provide crucial information for developers to fix the bugs. A “bad” bug report could cause a delay in bug fixing. Our proposed approach also relies on the quality of bug reports. If a bug report does not provide enough information, or provides misleading information, the performance of BLuAMIR is adversely affected.

\textbf{External Validity:} 
The nature of the data in open source projects may be different from those in projects developed by well-managed software organizations. We need to evaluate if our solution can be directly applied to commercial projects. We leave this as a future work. Then we will perform statistical tests to show that the improvement of our approach is statistically significant.

\textbf{Construct Validity}
In our experiment, we use three evaluation metrics, i.e., Top N rank, MAP and
MRR, and one statistical test, i.e., Wilcoxon signed-rank test. These metrics have been widely used before to evaluate previous approaches \cite{Jian, Saha} and are well- known IR metrics. Thus, we argue that our research has strong construct validity.

\textbf{Reliability:}
In our experiment section, we performed numerous experiments using various combinations of weighting functions to find the optimum parameters and the best accuracy of bug localization. The optimized $\alpha$, $\beta$, and $\gamma$ values are based on our experiments and are only for our proposed tool BLuAMIR. To automatically optimize control parameters for target projects, in the future we will expand our proposed approach using machine learning methods or generic algorithms.

\section{Related Work}\label{relatedwork}
There are many bug localization approaches proposed so far. They can be broadly categorized into two types - dynamic and static techniques. Generally, dynamic approaches can localize a bug much more precisely than static approaches. These techniques usually contrast the program spectra information (such as execution statistics) between passed and failed executions to compute the fault suspiciousness of individual program elements (such as statements, branches, and predicates), and rank these program elements by their fault suspiciousness. Developers may then locate faults by examining a list of program elements sorted by their suspiciousness. Some of the well known dynamic approaches are spectrum-based fault localization, e.g., \cite{Abreu,Jones,Lucia,SahaFault}, model-based fault localization, e.g., \cite{Feldman,Mayer}, dynamic slicing \cite{Zhang:2005}, delta debugging \cite{Zeller:2002}. 

Static approaches, on the other hand, do not require any program test cases or execution traces. In most cases, they need only program source code and bug reports. They are also computationally efficient. The static approaches usually can be categorized into two groups: program analysis based approaches and IR-based approaches. FindBugs is a program analysis based approach that locates a bug based on some predefined bug patterns \cite{FindBug}. Therefore, FindBug does not even need a bug report. However, it often detects too many false positives and misses many real bugs \cite{Tang}. IR-based approaches use information retrieval techniques (such as, TFIDF, LSA, LDA, etc.) to calculate the similarity between a bug report and a source code file. There are three traditionally-dominant IR paradigms TF.IDF \cite{Salton}, the “probabilistic approach” known as BM25 \cite{Robertson}, or more recent language modeling \cite{Ponte}. Another empirical study \cite{Fang} show that all three approaches perform comparably when well-tuned. However, \citet{Rao} investigates many standard information retrieval techniques for bug localization and find that simpler techniques, e.g., TFIDF and SUM, perform the best. 

In contrast with shallow “bag-of-words” models, latent semantic indexing (LSI) induces latent concepts. While a probabilistic variant of LSI has been devised \cite{Hofmann}, its probability model was found to be deficient.
%On the other hand, \citet{Lukins} created a Latent Dirichlet Allocation (LDA) model from the source code which provided word-topic modeling and topic-document distribution.
\citet{Lukins2} use Latent Dirichlet Allocation (LDA), which is a well-known topic modeling approach, to localize bug \cite{Lukins2}.
However, LSI is rarely used in practice today due to errors in induced concepts introducing more harm than good \cite{Hofmann} and LDA is not be able to predict the appropriate topic because it followed a generative topic model in a probabilistic way \cite{Lukins}.

\citet{Sisman} propose a history-aware IR-based bug localization solution to achieve a better
result. \citet{Jian} propose BugLocator, which leverages similarities among bug reports and uses refined vector space model to perform bug localization. \citet{Saha} build BLUiR that consider the structure of bug reports and source code files and employ structured retrieval to achieve a better result. \citet{Moreno} uses a text retrieval based technique and stack trace analysis to perform bug localization. To locate buggy files, they combines the textual similarity between a bug report and a code unit and the structural similarity between the stack trace and the code unit. Different from the existing IR-based bug localization approaches, \citet{Wang} propose AmaLgam, a new method for locating relevant buggy files that put together version history, similar report, and structure, to achieve better performance. Later \cite{Wang2} also propose AmaLgam+, which is a method for locating relevant buggy files that puts together fives sources of information i.e., version history, similar reports, structure, stack traces, and reporter information.
In our proposed technique BLuAMIR, we use version history, similar report and association relationship.






\section{Conclusion and Future Work} \label{sec:conclusionANDfuture}
During software evolution of a system, a large number of bug reports are submitted. For a large software project, developers must may need to examine a large number of source code files in order to locate the buggy files responsible for a bug, which is a tedious and expensive work. In this paper, we propose BLuAMIR, a new method for locating relevant buggy files that combines historical data, similar report, and keyword-source association map to achieve a higher accuracy. We perform a large-scale experiments on four projects, namely Eclipse, SWT and ZXing to localize more than 3,000 bugs. Our experiment of those dataset show that our technique can locate buggy files with a Top-10 accuracy of 64.05\% and a mean reciprocal rank@10 of 0.31 and a mean precision average@10 of 37\%, which are highly promising. We also compare our technique with state-of-the-art IR-based bug localization technique i.e., BugLocator. This also confirms superiority of our technique.

In the future, we will explore if several other bug related information such as bug report structure, source code structure, stack traces, reporter information can be integrated into our approach in order to improve bug localization performance. We would also like to reduce the threats to external validity further by applying our approach on more bug reports collected from other software systems.

\bibliographystyle{plainnat}
\scriptsize
\bibliography{test}
%\begin{thebibliography}{00}
%\bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
%\bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
%\bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
%\bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
%\bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
%\bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
%\bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
%\end{thebibliography}



\end{document}
